"""
Real-Time Blockchain Data Processor (Optimized)
Process Ethereum data in real-time with parallel execution
3 output modes: Console, JSON file, or Custom webhook
Performance: ~1000 tx/sec on modern hardware
"""
import os
import json
import logging
import time
import pandas as pd
from datetime import datetime
from web3 import Web3
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from etl.extract import extract_blocks
from etl.transform import transform_data
from concurrent.futures import ThreadPoolExecutor

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Configuration
RPC_URL = os.getenv('RPC_URL', "https://eth-mainnet.g.alchemy.com/v2/G09aLwdbZ-zyer6rwNMGu")
BATCH_SIZE = int(os.getenv('BATCH_SIZE', '5'))
POLLING_INTERVAL = int(os.getenv('POLLING_INTERVAL', '10'))  # Reduced from 60 to 10 seconds
MAX_WORKERS = int(os.getenv('MAX_WORKERS', '5'))  # Parallel threads
OUTPUT_MODE = os.getenv('OUTPUT_MODE', 'console')  # console, json, csv, webhook
WEBHOOK_URL = os.getenv('WEBHOOK_URL', '')


class RealtimeBlockchainProcessor:
    """Process blockchain data in real-time with parallel optimization"""
    
    def __init__(self):
        self.w3 = None
        self.last_block = None
        self.stats = {
            'blocks_processed': 0,
            'transactions_processed': 0,
            'total_eth_value': 0.0,
            'total_time': 0.0,
            'avg_time_per_block': 0.0
        }
        self.output_executor = ThreadPoolExecutor(max_workers=1)  # Single thread for I/O
    
    def initialize(self):
        """Initialize Web3 connection with retries"""
        max_retries = 5
        retry_delay = 5
        
        for attempt in range(max_retries):
            try:
                logger.info(f"üîå Connecting to RPC: {RPC_URL} (attempt {attempt + 1}/{max_retries})")
                self.w3 = Web3(Web3.HTTPProvider(RPC_URL, request_kwargs={'timeout': 30}))
                
                if not self.w3.is_connected():
                    logger.warning(f"Web3 connection failed on attempt {attempt + 1}")
                    if attempt < max_retries - 1:
                        logger.info(f"‚è≥ Retrying in {retry_delay}s...")
                        time.sleep(retry_delay)
                        continue
                    return False
                
                self.last_block = self.w3.eth.block_number
                logger.info(f"‚úÖ Connected to Ethereum - Current block: {self.last_block}")
                logger.info(f"‚öôÔ∏è  Polling interval: {POLLING_INTERVAL}s, Workers: {MAX_WORKERS}")
                return True
            except Exception as e:
                logger.error(f"Initialization failed (attempt {attempt + 1}): {e}")
                if attempt < max_retries - 1:
                    logger.info(f"‚è≥ Retrying in {retry_delay}s...")
                    time.sleep(retry_delay)
                else:
                    logger.error(f"‚ùå Failed to connect after {max_retries} attempts")
                    return False
        
        return False
    
    def process_realtime(self, continuous=True, interval=None):
        """
        Process blocks in real-time with parallel extraction.
        
        Args:
            continuous: Keep processing forever
            interval: Seconds to wait between checks (uses POLLING_INTERVAL env var if None)
        """
        if interval is None:
            interval = POLLING_INTERVAL
            
        logger.info(f"üöÄ Starting real-time processor (Output: {OUTPUT_MODE})")
        
        try:
            while True:
                batch_start = time.time()
                current_block = self.w3.eth.block_number
                
                # New blocks arrived
                if current_block > self.last_block:
                    start_block = self.last_block + 1
                    end_block = min(current_block, start_block + BATCH_SIZE - 1)
                    
                    logger.info(f"\nüì¶ Processing blocks {start_block}-{end_block} (Current: {current_block})")
                    
                    # Extract and transform with parallel processing
                    raw_data = extract_blocks(start_block, end_block, self.w3, parallel=True, max_workers=MAX_WORKERS)
                    
                    if raw_data:
                        transform_start = time.time()
                        processed_data = transform_data(raw_data)
                        transform_time = time.time() - transform_start
                        
                        if not processed_data.empty:
                            # Process in real-time (non-blocking output)
                            self.output_executor.submit(self._output_data, processed_data)
                            
                            # Update stats
                            self.stats['blocks_processed'] += (end_block - start_block + 1)
                            self.stats['transactions_processed'] += len(processed_data)
                            self.stats['total_eth_value'] += processed_data['value_eth'].sum()
                            
                            batch_time = time.time() - batch_start
                            self.stats['total_time'] += batch_time
                            self.stats['avg_time_per_block'] = self.stats['total_time'] / self.stats['blocks_processed']
                            
                            logger.info(f"‚úÖ Processed {len(processed_data)} transactions in {batch_time:.2f}s")
                            logger.info(f"üìä Transform: {transform_time:.3f}s | Total batch: {batch_time:.2f}s")
                            logger.info(f"üìà Avg: {self.stats['avg_time_per_block']:.2f}s/block")
                    
                    self.last_block = end_block
                else:
                    logger.debug(f"‚è≥ No new blocks (Last: {self.last_block}, Current: {current_block})")
                
                if not continuous:
                    break
                
                time.sleep(interval)
        
        except KeyboardInterrupt:
            logger.info("\n‚õî Stopped by user")
            self._print_summary()
        except Exception as e:
            logger.error(f"Error: {e}")
        finally:
            self.output_executor.shutdown(wait=True)
    
    def _output_data(self, df):
        """Output processed data based on mode"""
        if OUTPUT_MODE == 'console':
            self._output_console(df)
        elif OUTPUT_MODE == 'json':
            self._output_json(df)
        elif OUTPUT_MODE == 'csv':
            self._output_csv(df)
        elif OUTPUT_MODE == 'webhook':
            self._output_webhook(df)
    
    def _output_console(self, df):
        """Print to console (streaming view)"""
        print("\n" + "="*100)
        for idx, row in df.iterrows():
            print(f"""
Transaction #{idx + 1}:
  Block: {row['block_number']}
  From: {row['from_address'][:10]}...
  To:   {row['to_address'][:10] if pd.notna(row['to_address']) else 'Contract Creation'}...
  Value: {row['value_eth']} ETH
  Gas Used: {row['gas_used']}
  Status: {'‚úÖ Success' if row['status'] == 1 else '‚ùå Failed'}
            """)
        print("="*100)
    
    def _output_json(self, df):
        """Append to JSON file (streaming)"""
        filename = f"realtime_data_{datetime.now().strftime('%Y%m%d')}.json"
        
        records = json.loads(df.to_json(orient='records'))
        
        # Append to file
        try:
            with open(filename, 'r') as f:
                existing = json.load(f)
        except (FileNotFoundError, json.JSONDecodeError):
            existing = []
        
        existing.extend(records)
        
        with open(filename, 'w') as f:
            json.dump(existing, f, indent=2)
        
        logger.info(f"üíæ Saved {len(records)} records to {filename}")
    
    def _output_csv(self, df):
        """Append to CSV file (streaming)"""
        filename = f"realtime_data_{datetime.now().strftime('%Y%m%d')}.csv"
        
        # Append mode
        mode = 'a' if os.path.exists(filename) else 'w'
        df.to_csv(filename, mode=mode, header=(mode == 'w'), index=False)
        
        logger.info(f"üíæ Saved {len(df)} records to {filename}")
    
    def _output_webhook(self, df):
        """Send to webhook (e.g., Discord, Slack, custom API)"""
        if not WEBHOOK_URL:
            logger.warning("‚ö†Ô∏è No WEBHOOK_URL configured")
            return
        
        try:
            import requests
            
            # Aggregate data
            summary = {
                'timestamp': datetime.now().isoformat(),
                'transactions': len(df),
                'total_eth': float(df['value_eth'].sum()),
                'data': json.loads(df.to_json(orient='records'))
            }
            
            response = requests.post(WEBHOOK_URL, json=summary, timeout=5)
            logger.info(f"‚úÖ Webhook sent (status: {response.status_code})")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Webhook failed: {e}")
    
    def _print_summary(self):
        """Print final statistics"""
        print("\n" + "="*50)
        print("üìä REALTIME PROCESSOR SUMMARY")
        print("="*50)
        print(f"Blocks processed: {self.stats['blocks_processed']}")
        print(f"Transactions processed: {self.stats['transactions_processed']}")
        print(f"Total ETH value: {self.stats['total_eth_value']:.2f}")
        print("="*50)


def main():
    """Run real-time processor"""
    processor = RealtimeBlockchainProcessor()
    
    if not processor.initialize():
        return
    
    # Process continuously (Ctrl+C to stop)
    processor.process_realtime(continuous=True, interval=30)


if __name__ == "__main__":
    main()
